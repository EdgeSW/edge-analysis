{
 "metadata": {
  "name": "make_model"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Introduction:\n",
      "\n",
      "In order to grade physicians\u2019 performance on specific tasks a model of good and novice physicians must be created.  The specific steps to create this model are:  \n",
      "1.  Vector Quantization: reducing the total number of dimensions and normalizing the data  \n",
      "2.  Feature Segmentation: looking only at data while grasping objects  \n",
      "3.  Push to HMM training   "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "sys.path.append('C:\\\\Users\\\\Tyler\\\\.ipython\\\\edge-analysis')\n",
      "from scipy.cluster import vq\n",
      "import numpy as np\n",
      "import json, copy\n",
      "import scipy\n",
      "import nltk\n",
      "\n",
      "from datetime import datetime\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.cluster as cluster\n",
      "from HMM.data_wrangling import *\n",
      "import HMM.myBigQuery as myBQ\n",
      "import HMM.scrub as scrub\n",
      "import HMM.segment as segment"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Get Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "features = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Define labels, initial variables\n",
      "print 'getting features'\n",
      "dataset = 'Vel_dRta_Gr_dQg'\n",
      "task = 'pegtransfer'\n",
      "size = size_codebook(task)\n",
      "# Final Features are Vel-dRta-Gr-dQg (dX, dY, dZ, dRta, Fg, Qg, dQg)\n",
      "feats = ['dX','dY','dZ','dRta','Fg','Qg','dQg']\n",
      "hand = 'right'\n",
      "idx = {}; \n",
      "for n in feats: idx[n] = feats.index(n)\n",
      "\n",
      "keys = scrub.gtExp_keys[task]\n",
      "\n",
      "try:\n",
      "    features\n",
      "except NameError:\n",
      "    features = None\n",
      "if features is None:\n",
      "    print 'Loading data...'\n",
      "    features = getFeatures(dataset, sensors=feats, task=task, hand=hand, keys=keys, asdict=True)\n",
      "    print 'returned features of size', len(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "getting features\n",
        "Loading data..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SELECT key, dX, dY, dZ, dRta, Fg, Qg, dQg FROM [data.Vel_dRta_Gr_dQg] WHERE task='pegtransfer' AND hand='right' AND (key='pegtransfer_47' OR key='pegtransfer_168' OR key='pegtransfer_449' OR key='pegtransfer_9' OR key='pegtransfer_413' OR key='pegtransfer_18') \n",
        "current length: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 13171\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "returning data\n",
        "returned features of size"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Get VQ Codebook\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cdbkL, cdbkR, a, b = vqr.download_codebook('10-OCT-2012 1:33')\n",
      "pp = '.ipython/HMM-Train/codebooks/'+'_'.join(['cdbk','timdata',dataset,task,hand,'v1']) ##LOCAL\n",
      "#pp = 'codebooks/'+'_'.join(['cdbk','timdata',dataset,task,hand,'v1']) ##UBUNTU\n",
      "\n",
      "fh = open(pp,'r')\n",
      "codebook = np.array(json.loads(fh.read()))\n",
      "fh.close()\n",
      "print 'codebook size',len(codebook)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "codebook size 57\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Segmentation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get indexes of grasps\n",
      "#NOTE - force can start above 3.\n",
      "#Create new dict with keys as filenames and list of np.arrays that are the segments\n",
      "feat_seg = segment.segmentDictOfTestsAsList(features, task, idx['Fg'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Plot all segments separated by test if you'd like...\n",
      "for k, v in segment.segmentDictOfTests(features, task, idx['Fg']).iteritems():\n",
      "    a = plt.figure()\n",
      "    for seg in v:\n",
      "        a = plt.plot(seg[:,idx['Fg']])\n",
      "    a = plt.plot(range(250), [3]*250)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Plot all segments if you like...\n",
      "for segment in feat_seg:\n",
      "    a = plt.plot(segment[:, idx['Fg']])\n",
      "a = plt.plot(range(250), [3]*250)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Scrub, Normalize Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Scrub and normalize data for training\n",
      "### Butter, Holo ...(data is already filtered, derived)\n",
      "### Remove outliers???\n",
      "### Normalize\n",
      "\n",
      "#retrieve thresholds \n",
      "pp = '.ipython/HMM-Train/thresholds/'+'_'.join(['thresholds','timdata',task,hand]) ##LOCAL\n",
      "#pp = 'thresholds/'+'_'.join(['thresholds','timdata',task,hand]) ##UBUNTU\n",
      "\n",
      "fh = open(pp,'r')\n",
      "thresholds = json.loads(fh.read())\n",
      "fh.close()\n",
      "print 'Loaded thresholds size',len(thresholds)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loaded thresholds size 7\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def normalizeDictSegments(feat_seg, thresholds):\n",
      "    '''Return new dict of normalized, segmented features for each test. thresholds\n",
      "provided must be same as ones used for feat normalization prior to cdbk training'''\n",
      "    feat_seg_norm = copy.deepcopy(feat_seg)\n",
      "    for key, test in feat_seg_norm.iteritems():\n",
      "        for seg in test:\n",
      "            seg = scrub.normalizeByColumn(seg, thresholds)\n",
      "    return feat_seg_norm\n",
      "\n",
      "def normalizeListSegments(feat_seg, thresholds):\n",
      "    '''Return new list of normalized, segmented features for each test. thresholds\n",
      "provided must be same as ones used for feat normalization prior to cdbk training'''\n",
      "    feat_seg_norm = copy.deepcopy(feat_seg)\n",
      "    \n",
      "    return [scrub.normalizeByColumn(segment, thresholds) for segment in feat_seg_norm]\n",
      "\n",
      "\n",
      "feat_seg_norm = normalizeListSegments(feat_seg, thresholds)\n",
      "\n",
      "print feat_seg_norm[0][:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[-0.66112957  0.39804531 -0.93979933  0.8873592  -0.49401602 -0.81827353\n",
        "   1.90120968]\n",
        " [-0.60575858  0.52043536 -0.94831256  0.70713392 -0.24244835 -0.84255957\n",
        "   1.87432796]\n",
        " [-0.51937984  0.61928032 -0.91365157  0.53775553 -0.02284456 -0.85722006\n",
        "   1.4280914 ]\n",
        " [-0.40826873  0.67258996 -0.82973548  0.42678348  0.1221768  -0.85992218\n",
        "   0.73790323]\n",
        " [-0.28202289  0.66503776 -0.69717239  0.35002086  0.17781765 -0.85134746\n",
        "   0.06233199]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Encode Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "encoded_segments = scrub.encodeSegments(feat_seg_norm, codebook)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Take a visual peek at the distribution of codewords\n",
      "a,b,c = hist(np.concatenate(tuple([seg for seg in encoded_segments])), size)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Train Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Client & View:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''from IPython.parallel import Client\n",
      "#Start up IPClient\n",
      "print 'starting client'\n",
      "from IPython.parallel import Client\n",
      "ipclient = Client('/home/ubuntu/.starcluster/ipcluster/simcluster-us-east-1.json'\n",
      "            ,sshkey='/home/ubuntu/.ssh/simcluster.rsa'\n",
      "            ,packer='pickle')\n",
      "ipview = ipclient[:]\n",
      "'''\n",
      "'''def getClient():\n",
      "    ipclient = Client('/home/ubuntu/.starcluster/ipcluster/simcluster-us-east-1.json'\n",
      "                      ,sshkey='/home/ubuntu/.ssh/starcluster.rsa'\n",
      "                      ,packer='pickle')\n",
      "    ipview = ipclient[:]\n",
      "    return ipview, ipclient\n",
      "\n",
      "ipview, ipclient = getClient()\n",
      "print ipview\n",
      "print ipclient\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "\"def getClient():\\n    ipclient = Client('/home/ubuntu/.starcluster/ipcluster/simcluster-us-east-1.json'\\n                      ,sshkey='/home/ubuntu/.ssh/starcluster.rsa'\\n                      ,packer='pickle')\\n    ipview = ipclient[:]\\n    return ipview, ipclient\\n\\nipview, ipclient = getClient()\\nprint ipview\\nprint ipclient\\n\""
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rrr='''s = \"\"\"\"Your humble writer knows a little bit about a lot of things, but despite writing a fair amount about text processing (a book, for example), linguistic processing is a relatively novel area for me. Forgive me if I stumble through my explanations of the quite remarkable Natural Language Toolkit (NLTK), a wonderful tool for teaching, and working in, computational linguistics using Python. Computational linguistics, moreover, is closely related to the fields of artificial intelligence, language/speech recognition, translation, and grammar checking.\\nWhat NLTK includes\\nIt is natural to think of NLTK as a stacked series of layers that build on each other. Readers familiar with lexing and parsing of artificial languages (like, say, Python) will not have too much of a leap to understand the similar -- but deeper -- layers involved in natural language modeling.\\nGlossary of terms\\nCorpora: Collections of related texts. For example, the works of Shakespeare might, collectively, by called a corpus; the works of several authors, corpora.\\nHistogram: The statistic distribution of the frequency of different words, letters, or other items within a data set.\\nSyntagmatic: The study of syntagma; namely, the statistical relations in the contiguous occurrence of letters, words, or phrases in corpora.\\nContext-free grammar: Type-2 in Noam Chomsky's hierarchy of the four types of formal grammars. See Resources for a thorough description.\\nWhile NLTK comes with a number of corpora that have been pre-processed (often manually) to various degrees, conceptually each layer relies on the processing in the adjacent lower layer. Tokenization comes first; then words are tagged; then groups of words are parsed into grammatical elements, like noun phrases or sentences (according to one of several techniques, each with advantages and drawbacks); and finally sentences or other grammatical units can be classified. Along the way, NLTK gives you the ability to generate statistics about occurrences of various elements, and draw graphs that represent either the processing itself, or statistical aggregates in results.\\nIn this article, you'll see some relatively fleshed-out examples from the lower-level capabilities, but most of the higher-level capabilities will be simply described abstractly. Let's now take the first steps past text processing, narrowly construed. \"\"\"\n",
      "sentences = s.split('.')[:-1]\n",
      "seq = [map(lambda x:(x,''), ss.split(' ')) for ss in sentences]\n",
      "symbols = list(set([ss[0] for sss in seq for ss in sss]))\n",
      "states = range(5)\n",
      "trainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=states,symbols=symbols)\n",
      "m = trainer.train_unsupervised(seq)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Zip encoded segments into appropriate format\n",
      "encoded_segments = scrub.zipSegments(encoded_segments)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = datetime.now()\n",
      "print 'Training', task, ': \\n', start\n",
      "\n",
      "\n",
      "nltkTrainer = nltk.tag.hmm.HiddenMarkovModelTrainer(states=range(15), symbols=range(codebook.shape[0]))\n",
      "trained = nltkTrainer.train_unsupervised(encoded_segments, max_iterations=5)\n",
      "\n",
      "end = datetime.now()\n",
      "print end\n",
      "print end - start"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training pegtransfer : \n",
        "2013-01-28 14:23:47.668000\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0 logprob -43169.2189948\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1 logprob -33914.3322962\n",
        "iteration"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2 logprob -33914.3322962\n",
        "2013-01-28 14:27:19.512000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0:03:31.844000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trained_3_iterations = trained\n",
      "#4 minutes, logprob -37127"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "# now create a file\n",
      "# replace filename with the file you want to create\n",
      "file = open('model_timdata_'+task+'_'+hand, 'w')\n",
      "# now let's pickle picklelist\n",
      "pickle.dump(trained,file)\n",
      "# close the file, and your pickling is complete\n",
      "file.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###\n",
      "unpicklefile = open('model_timdata_'+task+'_'+hand, 'r')\n",
      "# now load the list that we pickled into a new object\n",
      "unpickledlist = pickle.load(unpicklefile)\n",
      "# close the file, just for safety\n",
      "unpicklefile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    }
   ],
   "metadata": {}
  }
 ]
}